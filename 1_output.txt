4,2.64258|
The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/tyang/tai_surf/LPTF_original/VMC_with_LPTFs/tvmc/models/PTF.py", line 485, in off_diag_labels_summed
            tgt=self.pe(self.tokenize(fsample))
            #grab your transformer output
            out,_=self.transformer.next_with_cache(tgt,cache[:,:N//self.p],N//self.p)
                  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    
            # grab output for the new part
  File "/home/tyang/tai_surf/LPTF_original/VMC_with_LPTFs/tvmc/models/PTF.py", line 78, in next_with_cache
            src = src + layer.dropout1(src2)
            src = layer.norm1(src)
            src2 = layer.linear2(layer.dropout(layer.activation(layer.linear1(src))))
                                               ~~~~~~~~~~~~~~~~ <--- HERE
            src = src + layer.dropout2(src2)
            src = layer.norm2(src)
  File "/home/tyang/.local/lib/python3.9/site-packages/torch/nn/functional.py", line 1471, in relu
        result = torch.relu_(input)
    else:
        result = torch.relu(input)
                 ~~~~~~~~~~ <--- HERE
    return result
RuntimeError: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 23.68 GiB of which 3.27 GiB is free. Including non-PyTorch memory, this process has 20.41 GiB memory in use. Of the allocated memory 12.75 GiB is allocated by PyTorch, and 6.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

